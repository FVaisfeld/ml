{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_comparison.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxMWx52XsgyPlyA5cgF78h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FVaisfeld/ml/blob/main/data_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check for bias between chinese and our data\n",
        "\n",
        "In this lab, I will check if we need to do some fine tuning or if we can just use the chinese data for training an arrythia model on out data. \n",
        "\n"
      ],
      "metadata": {
        "id": "UwCCv7NJ3JUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install this package to use Colab's GPU for training\n",
        "#!apt install --allow-change-held-packages libcudnn8=8.4.1.50-1+cuda11.6"
      ],
      "metadata": {
        "id": "LIwErZyD3NBb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below to load the dataset `Chinese_Cardisio_Combined.mat` \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "n73avgtQ3Um2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#annots.keys()\n",
        "#import numpy as np\n",
        "#import h5py\n",
        "#f = h5py.File('/content/Meta_Data.mat','r')\n",
        "#f.keys()\n",
        "\n",
        "import tables\n",
        "file = tables.open_file('/content/Measurements.mat')\n",
        "\n",
        "#import scipy.io\n",
        "#mat = scipy.io.loadmat('/content/Meta_Data.mat')\n",
        "#print(mat)\n",
        "#mat.keys()\n",
        "\n",
        "#data = np.array(data) # For converting to a NumPy array\n",
        "#annots[‘annotations’][0][0][‘bbox_x1’], annots[‘annotations’][0][0][‘fname’]\n",
        "#data = [[row.flat[0] for row in line] for line in annots[‘annotations’][0]]\n",
        "#columns = [‘bbox_x1’, ‘bbox_y1’, ‘bbox_x2’, ‘bbox_y2’, ‘class’, ‘fname’]\n",
        "#df_train = pd.DataFrame(data, columns=columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "LjKxQyMi3miB",
        "outputId": "feb98331-cbcd-461f-8ac5-0d65adf62e2c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HDF5ExtError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHDF5ExtError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-1afcba2cf83b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Measurements.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#import scipy.io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tables/file.py\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;31m# Finally, create the File instance, and return it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_uep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tables/file.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;31m# Now, it is time to initialize the File extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;31m# Check filters and set PyTables format version for new files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tables/hdf5extension.pyx\u001b[0m in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mHDF5ExtError\u001b[0m: HDF5 error back trace\n\n  File \"H5F.c\", line 620, in H5Fopen\n    unable to open file\n  File \"H5VLcallback.c\", line 3501, in H5VL_file_open\n    failed to iterate over available VOL connector plugins\n  File \"H5PLpath.c\", line 578, in H5PL__path_table_iterate\n    can't iterate over plugins in plugin path '(null)'\n  File \"H5PLpath.c\", line 620, in H5PL__path_table_iterate_process_path\n    can't open directory: /usr/local/hdf5/lib/plugin\n  File \"H5VLcallback.c\", line 3351, in H5VL__file_open\n    open failed\n  File \"H5VLnative_file.c\", line 97, in H5VL__native_file_open\n    unable to open file\n  File \"H5Fint.c\", line 1990, in H5F_open\n    unable to read superblock\n  File \"H5Fsuper.c\", line 405, in H5F__super_read\n    file signature not found\n\nEnd of HDF5 error back trace\n\nUnable to open/create file '/content/Measurements.mat'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then unzip the archive using the [zipfile](https://docs.python.org/3/library/zipfile.html) module.\n",
        "\n"
      ],
      "metadata": {
        "id": "2joweq4Y3uVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Unzip the dataset\n",
        "local_zip = './horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./horse-or-human')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "2xSrR-7R3xjs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}